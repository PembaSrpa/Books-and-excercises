---
key : "chapter-12"
title : "Chapter 12"
date : "2026-01-27"
description : "Text Analysis and Generation."
tags : ["Python", "Text-Analysis", "Random", "Data-Structures"]
---

# Text Analysis and Generation

### What is the issue with generating a purely random sequence of words?

When you use `random.choice` on a list of words where every word has an equal probability of being selected, the resulting text lacks structure and realism. It fails to account for the fact that in natural language, some words (like "the" or "and") appear much more frequently than others.

### How can we improve randomness using word frequencies?

We can use "weighted" randomness. By using the frequencies from a word counter as weights, we ensure that common words are chosen more often.

### Which Python function handles weighted selection?

The `random.choices` function (with an 's') takes an optional `weights` argument. You can also specify the number of words to select using the parameter `k`.

```python
import random

words = ['reach', 'streets', 'edward', 'a', 'said', 'to']
weights = [1, 1, 1, 10, 5, 8]
random_words = random.choices(words, weights=weights, k=6)
print(' '.join(random_words))
```

### Why does weighted random selection still fail to make sense?

Even with weighted selection, there is no relationship between successive words. In a real sentence, the choice of a word depends heavily on the word that came before it. For example, an article like "the" is usually followed by a noun or an adjective, not a verb.

---

### What are bigrams, trigrams, and n-grams?

* **Bigram:** A sequence of two consecutive words.
* **Trigram:** A sequence of three consecutive words.
* **n-gram:** A sequence of $n$ consecutive words.

### How do we represent bigram frequencies in Python?

We use a dictionary where the keys are tuples of strings (representing the bigram) and the values are integers (representing the count).

### What is the "Sliding Window" technique?

The sliding window is a list that maintains a fixed number of words as you iterate through a text. For bigrams, the window size is 2. After processing a pair, you remove the oldest word to make room for the next one.



### How is the sliding window implemented in code?

```python
window = []

def process_word(word):
    window.append(word)
    if len(window) == 2:
        count_bigram(window)
        window.pop(0)

def count_bigram(bigram):
    key = tuple(bigram)
    if key not in bigram_counter:
        bigram_counter[key] = 1
    else:
        bigram_counter[key] += 1
```

### What are the most common bigrams usually found in English books?

Typically, they are pairs of function words, such as:
* ('of', 'the')
* ('in', 'the')
* ('it', 'was')
* ('to', 'the')

---

### What is Markov Chain text analysis?

Markov analysis involves mapping each word in a text to a list of all the words that follow it. This allows us to predict the next word based on the current one.

### How is a successor map structured?

It is a dictionary where each key is a single word, and the value is a list of every word that has ever followed it in the source text. If a word follows another word multiple times, it appears in the list multiple times to maintain its probability.

```python
successor_map = {}

def add_bigram(bigram):
    first, second = bigram
    if first not in successor_map:
        successor_map[first] = [second]
    else:
        successor_map[first].append(second]
```

### How do we generate text using a successor map?

1.  Start with an initial word.
2.  Look up that word in the `successor_map`.
3.  Randomly choose one word from its list of successors.
4.  Make the chosen word the new current word.
5.  Repeat the process.

### Why is `random.choice` used on the successor list instead of `random.choices`?

Since the successor list contains duplicate entries for words that appeared more frequently, `random.choice` naturally selects them with higher probability. This reflects the original distribution of the text.

---

### What are the six strategies for debugging difficult bugs?

1.  **Reading:** Re-reading code to check if it actually says what you intended.
2.  **Running:** Experimenting with small changes or adding "scaffolding" (print statements).
3.  **Ruminating:** Thinking about the specific type of error (syntax, runtime, or semantic).
4.  **Rubberducking:** Explaining the logic to a person or a rubber duck to find gaps in reasoning.
5.  **Retreating:** Undoing recent changes until you reach a working version of the program.
6.  **Resting:** Giving your brain a break to allow the subconscious to solve the problem.

### What is a common failure mode in the "Reading" strategy?

If you have a conceptual misunderstanding of how the code works, you can read it many times and never see the error because the mistake is in your mental model, not just a typo.

### Why do beginning programmers struggle with "Retreating"?

Many are reluctant to delete code they have worked hard on, even if it is wrong. A "Best-in-Class" practice is to save a copy of the broken program before stripping it down to a working state.

### How is debugging similar to experimental science?

You should form a hypothesis about what is wrong and then design a small test that would either prove or disprove that hypothesis.

---

### Comparison of Text Generation Methods

| Method | Logic | Result Quality |
| :--- | :--- | :--- |
| **Simple Random** | Equal chance for every word | Nonsensical |
| **Weighted Random** | Frequency-based chance | Better vocabulary, poor structure |
| **Bigram Sampling** | Randomly pick two-word pairs | Recognizable phrases |
| **Markov Analysis** | Probabilistic word-to-word transition | Sounds like real (but confusing) sentences |

### What is the next step to make Markov text even better?

Use a prefix of more than one word. By mapping bigrams or trigrams to their successors, the context is preserved over longer distances, leading to much more coherent text generation.

### Why is it important to use tuples as keys in these dictionaries?

Dictionary keys must be immutable. Lists are mutable and cannot be used as keys, so we must convert our "window" or "bigram" lists into tuples before storing them in `bigram_counter` or `successor_map`.

### How do we handle file cleaning during this analysis?

We typically iterate through the file line by line, split the lines into words, and apply a cleaning function to remove punctuation and convert text to lowercase.

```python
for line in open(filename):
    for word in split_line(line):
        word = clean_word(word)
        process_word_bigram(word)
```

### What is the role of the `pop(0)` method in the sliding window?

It removes the word at the front of the list. This shifts the window forward by one word, allowing the next word added to form a new pair with the word that was previously the "second" word in the window.

### How does the `repr()` function assist in debugging?

`repr()` shows the string representation of an object, including escape characters. This is vital for finding hidden whitespace, tabs (`\t`), or newline characters (`\n`) that might be causing unexpected behavior in text processing.

### Summary of Chapter 12 Concepts

* **Randomness:** Moving from uniform to weighted distribution.
* **Data Structures:** Using tuples as keys for multi-word analysis.
* **Algorithms:** Implementing sliding windows and successor maps.
* **Methodology:** Applying structured debugging to complex text-generation logic.

---

### Final Exercise: Generating a Sentence

1.  Analyze a source text (e.g., a book).
2.  Build a `successor_map`.
3.  Choose a starting word like "although".
4.  Generate 10 words by repeatedly sampling the map.
5.  Output the string: "although i a it the we they i ..."

### Why does the text still feel "off"?

While Markov chains capture local relationships (word A follows word B), they do not capture global structure (the beginning of a sentence related to the end). Higher-order n-grams help bridge this gap.

### How to manage large successor lists in memory?

For very large texts, storing every successor in a list can be memory-intensive. In those cases, a dictionary of dictionaries (mapping word $\rightarrow$ successor $\rightarrow$ count) is often more efficient.

### What should you do if you are stuck for more than an hour?

Switch to the **Resting** or **Rubberducking** strategy. Often, the act of verbalizing the problem or stepping away from the screen provides the clarity needed to spot a semantic error.

### Conclusion on Text Analysis

This chapter serves as a bridge between simple data storage and the complex probabilistic models used in modern Natural Language Processing (NLP). Understanding these basics is essential for anyone following the **Data Analyst Roadmap** into advanced Python and AI applications.

**Would you like me to help you implement the Exercise mentioned at the end of the chapter where you use a bigram as a key for the successor map?**
